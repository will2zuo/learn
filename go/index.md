## GMP 模型
go 语言的并发调度模型
- G（Goroutine）：轻量级用户态协程
- M（Machine）：操作系统线程，负责执行 G，M 是通过绑定 P 来执行 G
- P（Processor）：调度器或者逻辑处理器，管理 G 的本地队列（<=256），数量由 runtime.GOMAXPROCS 决定（默认等于 CPU 核数）

###  GMP 调度机制
- 初始化：启动 GOMACPROCS 个 P 和少量 M
- P 的本地队列和全局队列
  - P 的本地队列：新创建的 G 会优先加入 P 的本地队列，满时会分走一半到全局队列
  - 全局队列：存放的是等待运行的 G
- M绑定P：M需绑定P才能执行G，若M阻塞（如系统调用），会释放P并由其他M接管
- Work-Stealing 机制：若P的本地队列为空，P从全局队列或其他P的本地队列“窃取”一半G
![gmp.png](gmp.png)

### go func 的调度流程
![img.png](img.png)

### 调度器的生命周期
![img_1.png](img_1.png)

## Go 的垃圾回收机制
标记准备（stw）->并发标记（三色标记法+混合写屏障）->标记终止（stw）->并发清除->清除终止

### 核心机制：三色标记法
- 白色：未访问对象，需要被回收的对象
- 灰色：已访问但子对象未完全扫描
- 黑色：确认存活的对象
- *流程*
  - 从所有根对象【寄存器/全局变量/执行栈】开始遍历，可达的先标记为灰色
  - 遍历所有灰色，变为黑色，再将可达的白色变为灰色 
  - 重复第二步，直到遍历完所有对象，剩下的黑色则是可以存活的对象，白色的则是需要回收的对象
- 存在的问题
  - 并发修改引用对象，可能导致对象丢失
  - 存在 stw，gc 效率过低
- 解决方法：混合写屏障

### 混合写屏障
- 核心原理：保证三色不变性
  - 插入屏障：新增引用对象，如果是白色，则将其标记为灰色，防止黑色对象直接引用白色对象
  - 删除屏障：删除引用对象，如果是白色或者是灰色，则将其标记为灰色，防止引用断开导致白色对象被误回收
- 强三色不变式：既不存在黑色的对象引用白色的对象
- 弱三色不变式：既所有黑色引用到的白色对象，都有可达的路径，这个白色对象必须存在其他灰色对象对它的引用，或者可达它的链路上游存在灰色对象
- 混合写屏障流程：
  - 初始扫描（无 STW），优先扫描所有goroutine的栈，将栈上可达的对象直接标记为黑色（存活对象），并保持黑色状态不变
  - 堆对象标记和屏障启用
    - 堆上的对象按三色标记法处理（白→灰→黑），同时启用混合写屏障
    - 启用插入写屏障和删除写屏障规则
  - 并发标记阶段
    - 程序与GC并发执行，写屏障捕获所有堆对象的引用变更（新增或删除），启动插入和删除屏障保证能正确回收
  - 标记终止和清理
    - 标记完成之后，清理白色对象
    - 无需重新扫描栈，因栈对象始终为黑色，避免了传统插入写屏障所需的STW重新扫描

### 根对象是什么
又叫做根集合，在垃圾回收过程中最先检查的对象：
1. 全局变量
2. 执行栈
3. 寄存器

### 垃圾回收分类
1. 追踪式：从根对象出发，根据对象之间的引用关系，一步步推进知道扫描整个堆并确定要保留的对象，从而进行垃圾回收
2. 引用计数式：每个对象包含一个引用计数器，当引用计数器归零的时候就会被回收

### 触发 GC 的时机是什么时候
1. 手动触发： runtime.GC
2. 被动触发：使用系统监控，当一定时间【默认时间两分钟】没有 gc 时，强制触发；通过监控内存增长的比列来触发；如果没有开启 gc，则启动 gc

### 如果内存分配速度超过了标记清除的速度怎么办？
- 通过 GOGC 调整 GC 触发策略。
- 优化代码减少内存分配。
- 监控和分析内存使用，排查泄漏或低效代码。
- 在极端情况下，结合架构级优化（如分片、离线处理）降低内存压力。
### go 的 gc 如何调优
- 控制内存分配的速度，限制 goroutine 的数量，从而提高赋值器对 CPU 的利用率。
- 减少并复用内存，例如使用 sync.Pool 来复用需要频繁创建临时对象，例如提前分配足够的内存来降低多余的拷贝。
- 需要时，增大 GOGC 的值，降低 GC 的运行频率。

## Go Map 的实现
主要基于哈希表，由 hmap 和 bmap两个核心结构体组成，采用链表法解决哈希冲突，并通过动态扩容优化性能

基本数据结构

```go
type hmap struct {
    count     int // map中键值对的数量
    B         uint8 // 桶的数量，即哈希表中桶的个数
    noverflow uint16 // 溢出桶的数量
    hash0     uint32 // 哈希种子,它能为哈希函数的结果引入随机性，这个值在创建哈希表时确定，并在调用哈希函数时作为参数传入
    buckets   unsafe.Pointer // 桶数组的指针
    oldbuckets unsafe.Pointer // 旧桶数组的指针，用于扩容时的数据迁移
    nevacuate uintptr // 扩容时的标记位
    extra *mapextra // 附加信息，包括溢出桶和哈希表的状态等
}

type mapextra struct {
	overflow    *[]*bmap
	oldoverflow *[]*bmap
	nextOverflow *bmap
}

// 在编译期间会转化
type bmap struct {
    tophash [bucketCnt]uint8 // 存储哈希值的高8位
    keys    [bucketCnt]key   // 存储键的数组
    values  [bucketCnt]value // 存储值的数组
    overflow *bmap           // 溢出桶的指针
}
```
![img_4.png](img_4.png)
![img_2.png](img_2.png)
哈希表 runtime.hmap 的桶是 runtime.bmap。每一个 runtime.bmap 都能存储 8 个键值对，当哈希表中存储的数据过多，单个桶已经装满时就会使用 extra.nextOverflow 中桶存储溢出的数据。

桶的结构体 runtime.bmap 在 Go 语言源代码中的定义只包含一个简单的 tophash 字段，tophash 存储了键的哈希的高 8 位，通过比较不同键的哈希的高 8 位可以减少访问键值对次数以提高性能

随着哈希表存储的数据逐渐增多，我们会扩容哈希表或者使用额外的桶存储溢出的数据，不会让单个桶中的数据超过 8 个，不过溢出桶只是临时的解决方案，创建过多的溢出桶最终也会导致哈希的扩容


### map 初始化
#### 字面量
一般是通过 key：value 的方式
```go
hash := map[string]int{
	"1": 2,
	"3": 4,
	"5": 6,
}
```
当哈希表中的元素数量少于或者等于 25 个时，会将所有的键值对一次加入到哈希表中，当哈希表中元素的数量超过了 25 个，编译器会创建两个数组分别存储键和值，这些键值对会通过如下所示的 for 循环加入哈希

```go
// ageMp 为 nil，不能向其添加元素，会直接panic
var ageMp map[string]int
```

### map 的增删改查
- 增：根据hash算法查到对应的桶，如果桶没有存满，就顺序在后面存，如果存满了，则存入溢出桶，通过链表链接
- 查：根据hash算法找到对应的桶，再经过高8位找到对应的值返回，如果没有，就去溢出桶找，没有直接返回
- 删：delete
### map 中的 key 为什么是无序的
因为在遍历map 时，不是固定的从 0 的 bucket 开始遍历的，而是随机序号的 bucket 开始遍历的
### map 扩容
#### 什么时候扩容
- 装载因子大于 6.5 = 元素个数/桶的个数 => 元素太多，而桶太少
  - bucket 最大数量（2^B）直接变成原来 bucket 数量的 2 倍 (2^B *2)
- 存在太多的溢出桶
  - 等量扩容
  - 其实内存整理，清理过多的溢出桶
  - 怎么判断溢出桶太多
    - 桶 b 的个数小于 15
      - 溢出桶数量超过 2^B 
    - 桶 b 的个数大于 15
      - 溢出桶的数量超过 2^15

#### map 扩容是动态扩容
- 在调用写操作的时候增量扩容
- 从 oldbucket 迁移到 bucket 中
#### 可以对 map 边遍历边删除吗
- 不能，map 不是线程安全的；可以用 sync.RWMutex 或者 sync.Map 
#### 可以对两个 map比较吗
- 只有两个为 nil 的 map 可以比较
- 只能是遍历map 的每个元素，比较元素是否都是深度相等

### hash 算法
#### 开放寻址法
依次探测和对比目标键值是否在哈希表中
- 存在：会将值写在下一个索引为空的位置
- 不存在：就直接写到当前位置
#### 拉链法
通过 hash 算法找到一个桶
- 如果找到键相同的键值对，直接更新键值对
- 没有找到相同的键，直接在后面更新键值对，如果当前桶满了，就更新到溢出桶中
- 用的是链表做为底层的数据结构

#### float 类型可以作为 map 的 key 吗
- float 型可以作为 key，但是由于精度的问题，会导致一些诡异的问题，慎用之。

#### map 删除一个 key，它的内存会被释放吗？
不会，只会做一个标记 EmptyOne，如果 map == nil 的时候，才会被 gc 回收

## Go channel
### 数据结构
```go
type hchan struct {
  //channel分为无缓冲和有缓冲两种。
  //对于有缓冲的channel存储数据，借助的是如下循环数组的结构
	qcount   uint           // 循环数组中的元素数量
	dataqsiz uint           // 循环数组的长度
	buf      unsafe.Pointer // 指向底层循环数组的指针
	elemsize uint16 //能够收发元素的大小
  

	closed   uint32   //channel是否关闭的标志
	elemtype *_type //channel中的元素类型
  
  //有缓冲channel内的缓冲数组会被作为一个“环型”来使用。
  //当下标超过数组容量后会回到第一个位置，所以需要有两个字段记录当前读和写的下标位置
	sendx    uint   // 下一次发送数据的下标位置
	recvx    uint   // 下一次读取数据的下标位置
  
  //当循环数组中没有数据时，收到了接收请求，那么接收数据的变量地址将会写入读等待队列
  //当循环数组中数据已满时，收到了发送请求，那么发送数据的变量地址将写入写等待队列
	recvq    waitq  // 读等待队列
	sendq    waitq  // 写等待队列


	lock mutex //互斥锁，保证读写channel时不存在并发竞争问题
}
```
![img_3.png](img_3.png)

- 用来保存goroutine之间传递数据的循环链表。=====> buf。
- 用来记录此循环链表当前发送或接收数据的下标值。=====> sendx和recvx。
- 用于保存向该chan发送和从改chan接收数据的goroutine的队列。=====> sendq 和 recvq
- 保证channel写入和读取数据时线程安全的锁。 =====> lock

channel 出现 panic 场景：
- 向已经关闭的 channel 写数据
- 关闭已经关闭的 channel
- 关闭为 nil 的 channel

chan 出现阻塞的场景：
- 给 nil 的chan 发送数据
- 读取 nil 的 chan

## slice 的扩容机制
go.1.18 版本之前：原 slice 的容量小于 1024 的扩容 2 倍，大于 1024 的扩容 1.25 倍

新版本的 go，原容量小于 256 的，扩容 2倍，大于 256 的扩容 newcap = oldcap+(oldcap+3*256)/4

## go 中 make 和 new 的区别
都是用来分配内存的内建函数
- new： 分配空间后是将内存清零，并没有初始化内存；new 可以为每种类型分配内存；返回的是指向内存的指针
- make：分配空间之后，是初始化内存，不是清零；make 只用于 slice、map、chan 三种；make 返回的是类型；

## go context 的作用
- 上下文控制
- go goroutine 之间的数据交换
- 超时控制

## go 协程泄露的场景以及排查方法
- goroutine泄露，一般是没有被关闭或者没有添加超时控制，让 goroutine 一直阻塞，不能被 gc 回收
- 排查
  - 使用runtime.NumGoroutine()监控数量，结合pprof的goroutine profile生成堆栈跟踪。

## 如何减少GC对延迟敏感服务的影响？
- 降低分配频率，服用对象，避免频繁创建大对象
- 调整GC参数：设置GOGC（默认100，降低值减少堆增长，但增加GC频率）
- 监控工具：通过GODEBUG=gctrace=1输出GC日志，分析runtime.MemStats

## go 中 M 的自旋机制
用于在并发调度时减少线程的休眠和唤醒的开销，同时平衡 cpu资源的利用率，快速响应新任务 

### 自旋触发的条件
- 本地队列为空：当前 M 绑定的 P中没有可运行的 G
- 全局队列为空：全局队列中没有可以调度的 G
- 网络轮询器无就绪的 G
- 未超过自旋线程数的限制：运行时通过spinning计数器控制活跃的自旋M数量，避免过多CPU浪费。
### 自旋的行为
- 主动窃取（Work Stealing）：尝试从其他P的本地队列偷取G（通过runtime.stealWork实现），最多尝试4次（stealTries常量
- 检查全局队列和定时器：定期扫描全局队列和定时器任务，避免遗漏可执行的G
- 短暂空循环（Procyield）

### 自旋的终止
- 成功窃取到G：立即执行该G，并退出自旋状态。
- 自旋超时（约10μs）：若长时间未找到任务，M会休眠并释放P，等待唤醒
- 系统监控（sysmon）介入：若M自旋时间过长，sysmon线程会强制其休眠

### 设计目标与优化
- 减少线程切换​：自旋避免了M频繁休眠和唤醒的上下文切换开销
- 平衡CPU使用​：通过限制自旋M的数量（通常≤GOMAXPROCS/2），防止CPU空转
- 响应速度优先​：在低负载时，自旋能快速响应新创建的G，提升吞吐量

## 锁
### 互斥锁【sync.Mutex】，保护共享资源
- 保证同一时刻只有一个协程可以访问
- 底层实现
  - 维护 state 字段来表示锁的状态
  - 如果没有上锁，就直接获取
  - 如果被锁住了，协程先自旋几次，没有获得锁就进入休眠
  - 被阻塞的协程会加入一个等待队列，先入先出，等锁释放被唤醒

### 读写锁【RWMutex】
- 读可以同时，但是写是独占的
- 底层实现
  - 读锁和写锁分离，读锁可以多个协程同时占用， 但是写锁是独占的
  - 优先级：写锁有限
  - 适合读多写少的场景

### Once，确保某个操作只执行一次
- 底层实现
  - 通过原子操作和互斥锁实现
  - 适用一个标志位 done 记录是否已经执行
  - 单例化场景
    
### WaitGroup，等待一组协程执行完
- 底层实现
  - 通过计数器记录未完成任务

### Sync.map，并发安全的 map
- 底层实现
  - 维护两个 map，一个是只读 map，一个是读写 map
  - 读写分离
- 适合读多写少的场景

#### sync.map 怎么实现
1. 读写分离，一个 read，读的字段只读 read 上的； 和一个 dirty，新写的字段 在 dirty字段上
2. 读取的时候先读 read 里，如果没有在读 dirty
3. 读 read 的时候不需要加锁，读或者写 dirty 需要加锁
4. misses 字段来记录击穿的次数（misses = len（dirty）），达到一定次数就会将 dirty 的数据缓存到 read 里
5. 适合多读少写的场景
6. 对于删除是通过标记 amended 来延迟删除，是在 store 中删除

